{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "*Supervised* machine learning techniques involve training a model to operate on a set of *features* and predict a *label* using a dataset that includes some already-known label values. You can think of this function like this, in which ***y*** represents the label we want to predict and ***X*** represents the vector of features the model uses to predict it.\n",
    "\n",
    "$$y = f([x_1, x_2, x_3, ...])$$\n",
    "\n",
    "\n",
    "*Classification* is a form of supervised machine learning in which you train a model to use the features (the ***x*** values in our function) to predict a label (***y***) that calculates the probability of the observed case belonging to each of a number of possible classes, and predicting an appropriate label. The simplest form of classification is *binary* classification, in which the label is 0 or 1, representing one of two classes; for example, \"True\" or \"False\"; \"Internal\" or \"External\"; \"Profitable\" or \"Non-Profitable\"; and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification\n",
    "\n",
    "Let's start by looking at an example of *binary classification*, where the model must predict a label that belongs to one of two classes. In this exercsie, we'll train a binary classifier to predict whether or not a patient should be tested for diabetes based on some medical data.\n",
    "\n",
    "### Explore the data\n",
    "\n",
    "Run the following cell to load a CSV file of patent data into a **Pandas** dataframe:\n",
    "\n",
    "> **Citation**: The diabetes dataset used in this exercise is based on data originally collected by the National Institute of Diabetes and Digestive and Kidney Diseases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82, 13)\n",
      "(82, 13)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "problemreports = ['/Month End Reporting/Automated EOM Reports/Debit And Credit Balance Ttl by Service Group by account','/Month End Reporting/Automated EOM Reports/Payments in Total by Service - Corp']\n",
    "# load the training dataset\n",
    "reportrun = pd.read_csv('data/DebitCreditReportPrediction.csv')\n",
    "# reportrun = reportrun[reportrun['itempath'] ==  '/Month End Reporting/Automated EOM Reports/Payments in Total by Service - Corp']\n",
    "reportrun = reportrun[reportrun.itempath.isin(problemreports)]\n",
    "\n",
    "# reportrun = reportrun[(reportrun['DataRetrievalSeconds'] > 100)]\n",
    "# reportrun.reset_index(inplace = True)\n",
    "print(reportrun.shape)\n",
    "reportrun = reportrun.dropna()\n",
    "print(reportrun.shape)\n",
    "\n",
    "\n",
    "# '/Month End Reporting/Automated EOM Reports/Payments in Total by Service - Corp'\n",
    "# /Month End Reporting/Automated EOM Reports/Debit And Credit Balance Ttl by Service Group by account\n",
    "\n",
    "\n",
    "\n",
    "# reportrun.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 \n",
      "  Features: [406, 32, 0] \n",
      "  Label: 1.0\n",
      "Run 2 \n",
      "  Features: [658, 55, 2] \n",
      "  Label: 1.0\n",
      "Run 3 \n",
      "  Features: [212, 20, 11] \n",
      "  Label: 1.0\n",
      "Run 4 \n",
      "  Features: [32, 14, 0] \n",
      "  Label: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Separate features and labels\n",
    "features = ['DataRetrievalSeconds','TimeProcessingSeconds','RenderingSeconds']\n",
    "# features = ['RenderingSeconds']\n",
    "label = 'statuscat'\n",
    "X, y = reportrun[features].values, reportrun[label].values\n",
    "\n",
    "for n in range(0,4):\n",
    "    print(\"Run\", str(n+1), \"\\n  Features:\",list(X[n]), \"\\n  Label:\", y[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the feature distributions for each label value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "\n",
    "for col in features:\n",
    "    reportrun.boxplot(column=col, by='statuscat', figsize=(6,6))\n",
    "    plt.title(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data 70%-30% into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=10)\n",
    "\n",
    "print ('Training cases: %d\\nTest cases: %d' % (len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn. metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "print(kf)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    # Set regularization rate\n",
    "    reg = 0.01\n",
    "\n",
    "    # train a logistic regression model on the training set\n",
    "    model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print('Accuracy: ', accuracy_score(y_test, predictions))\n",
    "    print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate a Binary Classification Model\n",
    "OK, now we're ready to train our model by fitting the training features (**X_train**) to the training labels (**y_train**). There are various algorithms we can use to train the model. In this example, we'll use *Logistic Regression*, which (despite its name) is a well-established algorithm for classification. In addition to the training features and labels, we'll need to set a *regularization* parameter. This is used to counteract any bias in the sample, and help the model generalize well by avoiding *overfitting* the model to the training data.\n",
    "\n",
    "> **Note**: Parameters for machine learning algorithms are generally referred to as *hyperparameters* (to a data scientist, *parameters* are values in the data itself - *hyperparameters* are defined externally from the data!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set regularization rate\n",
    "reg = 0.01\n",
    "\n",
    "# train a logistic regression model on the training set\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've trained the model using the training data, we can use the test data we held back to evaluate how well it predicts. Again, **scikit-learn** can help us do this. Let's start by using the model to predict labels for our test set, and compare the predicted labels to the known labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "print('Predicted labels: ', predictions)\n",
    "print('Actual labels:    ' ,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy: ', accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn. metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report includes the following metrics for each class  (0 and 1)\n",
    "\n",
    "> note that the header row may not line up with the values!\n",
    "\n",
    "* *Precision*: Of the predictons the model made for this class, what proportion were correct?\n",
    "* *Recall*: Out of all of the instances of this class in the test dataset, how many did the model identify?\n",
    "* *F1-Score*: An average metric that takes both precision and recall into account.\n",
    "* *Support*: How many instances of this class are there in the test dataset?\n",
    "\n",
    "The classification report also icludes averages for these metrics, including a weighted average that allows for the imbalance in the number of cases of each class.\n",
    "\n",
    "Because this is a *binary* classification problem, the ***1*** class is considered *positive* and its precision and recall are particularly interesting - these in effect answer the questions:\n",
    "\n",
    "- Of all the patients the model predicted are diabetic, how many are actually diabetic?\n",
    "- Of all the ptients that are actually diabetic, how many did the model identify?\n",
    "\n",
    "You can retrieve these values on their own by using the **precision_score** and **recall_score** metrics in scikit-learn (which by default assume a binary classification model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(\"Overall Precision:\",precision_score(y_test, predictions))\n",
    "print(\"Overall Recall:\",recall_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision and recall metrics are derived from four possible prediction outcomes:\n",
    "* *True Positives*: The predicted label and the actual label are both 1.\n",
    "* *False Positives*: The predicted label is 1, but the actual label is 0.\n",
    "* *False Negatives*: The predicted label is 0, but the actual label is 1.\n",
    "* *True Negatives*: The predicted label and the actual label are both 0.\n",
    "\n",
    "These metrics are generally tabulated for the test set and shown together as a *confusion matrix*, which takes the following form:\n",
    "\n",
    "<table style=\"border: 1px solid black;\">\n",
    "    <tr style=\"border: 1px solid black;\">\n",
    "        <td style=\"border: 1px solid black;color: black;\" bgcolor=\"lightgray\">TN</td><td style=\"border: 1px solid black;color: black;\" bgcolor=\"white\">FP</td>\n",
    "    </tr>\n",
    "    <tr style=\"border: 1px solid black;\">\n",
    "        <td style=\"border: 1px solid black;color: black;\" bgcolor=\"white\">FN</td><td style=\"border: 1px solid black;color: black;\" bgcolor=\"lightgray\">TP</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Note that the correct (*true*) predictions form a diagonal line from top left to bottom right - these figures should be significantly higher than the *false* predictions if the model is any good.\n",
    "\n",
    "In Python, you can use the **sklearn.metrics.confusion_matrix** function to find these values for a trained classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Print the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print (cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we've considered the predictions from the model as being either 1 or 0 class labels. Actually, things are a little more complex than that. Statistical machine learning algorithms, like logistic regression, are based on *probability*; so what actually gets predicted by a binary classifier is the probability that the label is true (**P(y)**) and the probability that the label is false (1 - **P(y)**). A threshold value of 0.5 is used to decide whether the predicted label is a 1 (*P(y) > 0.5*) or a 0 (*P(y) <= 0.5*). You can use the **predict_proba** method to see the probability pairs for each case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_scores = model.predict_proba(X_test)\n",
    "print(y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision to score a prediction as a 1 or a 0 depends on the threshold to which the predicted probabilties are compared. If we were to change the threshold, it would affect the predictions; and therefore change the metrics in the confusion matrix. A common way to evaluate a classifier is to examine the *true positive rate* (which is another name for recall) and the *false positive rate* for a range of possible thresholds. These rates are then plotted against all possible thresholds to form a chart known as a *received operator characteristic (ROC) chart*, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "\n",
    "# plot ROC curve\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "# Plot the diagonal 50% line\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "# Plot the FPR and TPR achieved by our model\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC chart shows the curve of the true and false positive rates for different threshold values between 0 and 1. A perfect classifier would have a curve that goes straight up the left side and straight across the top. The diagonal line across the chart represents the probability of predicting correctly with a 50/50 random prediction; so you obviously want the curve to be higher than that (or your model is no better than simply guessing!).\n",
    "\n",
    "The area under the curve (AUC) is a value between 0 and 1 that quantifies the overall performance of the model. The closer to 1 this value is, the better the model. Once again, scikit-Learn includes a function to calculate this metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform preprocessing in a pipeline\n",
    "\n",
    "In this case, the ROC curve and its AUC indicate that the model performs better than a random guess which is not bad considering we performed very little preprocessing of the data.\n",
    "\n",
    "In practice, it's common to perform some preprocessing of the data to make it easier for the algorithm to fit a model to it. There's a huge range of preprocessing trasformations you can perform to get your data ready for modeling, but we'll limit ourselves to a few common techniques:\n",
    "\n",
    "- Scaling numeric features so they're on the same scale. This prevents feaures with large values from producing coefficients that disproportionately affect the predictions.\n",
    "- Encoding categorical variables. For example, by using a *one hot encoding* technique you can create individual binary (true/false) features for each possible category value.\n",
    "\n",
    "To apply these preprocessing transformations, we'll make use of a Scikit-Learn feature named *pipelines*. These enable us to define a set of preprocessing steps that end with an algorithm. You can then fit the entire pipeline to the data, so that the model encapsulates all of the preprocessing steps as well as the regression algorithm. This is useful, because when we want to use the model to predict values from new data, we need to apply the same transformations (based on the same statistical distributions and catagory encodings used with the training data).\n",
    "\n",
    ">**Note**: The term *pipeline* is used extensively in machine learning, often to mean very different things! In this context, we're using it to refer to pipeline objects in Scikit-Learn, but you may see it used elsewhere to mean someting else.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Define preprocessing for numeric columns (normalize them so they're on the same scale)\n",
    "numeric_features = [0,1,2,3,4,5,6]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Define preprocessing for categorical features (encode the Age column)\n",
    "categorical_features = [7]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Create preprocessing and training pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('logregressor', LogisticRegression(C=1/reg, solver=\"liblinear\"))])\n",
    "\n",
    "\n",
    "# fit the pipeline to train a logistic regression model on the training set\n",
    "model = pipeline.fit(X_train, (y_train))\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline encapsulates the preprocessing steps as well as model training.\n",
    "\n",
    "Let's use the model trained by this pipeline to predict labels for our test set, and compare the performance metrics with the basic model we created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get predictions from test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Get evaluation metrics\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print ('Confusion Matrix:\\n',cm, '\\n')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions))\n",
    "print(\"Overall Precision:\",precision_score(y_test, predictions))\n",
    "print(\"Overall Recall:\",recall_score(y_test, predictions))\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))\n",
    "\n",
    "# calculate ROC curve\n",
    "y_scores = model.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "\n",
    "# plot ROC curve\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "# Plot the diagonal 50% line\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "# Plot the FPR and TPR achieved by our model\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look a little better, so clearly preprocessing the data has made a difference.\n",
    "\n",
    "### Try a different algorithm\n",
    "\n",
    "Now let's try a different algorithm. Previously we used a logistic regression algorithm, which is a *linear* algorithm. There are many kinds of classification algorithm we could try, including:\n",
    "\n",
    "- **Support Vector Machine algorithms**: Algorithms that define a *hyperplane* that separates classes.\n",
    "- **Tree-based algorithms**: Algorithms that build a decision tree to reach a prediction\n",
    "- **Ensemble algorithms**: Algorithms that combine the outputs of multiple base algorithms to improve generalizability.\n",
    "\n",
    "This time, We'll use the same preprocessing steps as before, but we'll train the model using an *ensemble* algorithm named *Random Forest* that combines the outputs of multiple random decision trees (for more details, see the [Scikit-Learn documentation](https://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create preprocessing and training pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('logregressor', RandomForestClassifier())])\n",
    "\n",
    "# fit the pipeline to train a random forest model on the training set\n",
    "model = pipeline.fit(X_train, (y_train))\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the performance metrics for the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print ('Confusion Matrix:\\n',cm, '\\n')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions))\n",
    "print(\"Overall Precision:\",precision_score(y_test, predictions))\n",
    "print(\"Overall Recall:\",recall_score(y_test, predictions))\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('\\nAUC: ' + str(auc))\n",
    "\n",
    "# calculate ROC curve\n",
    "y_scores = model.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "\n",
    "# plot ROC curve\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "# Plot the diagonal 50% line\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "# Plot the FPR and TPR achieved by our model\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks better!\n",
    "\n",
    "### Use the Model for Inferencing\n",
    "Now that we have a reasonably useful trained model, we can save it for use later to predict labels for new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model as a pickle file\n",
    "filename = './models/diabetes_model.pkl'\n",
    "joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have some new observations for which the label is unknown, we can load the model and use it to predict values for the unknown label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the model from the file\n",
    "model = joblib.load(filename)\n",
    "\n",
    "# predict on a new sample\n",
    "# The model accepts an array of feature arrays (so you can predict the classes of multiple patients in a single call)\n",
    "# We'll create an array with a single array of features, representing one patient\n",
    "X_new = [[2,180,74,24,21,23.9091702,1.488172308,22]]\n",
    "print ('New sample: {}'.format(list(X_new[0])))\n",
    "\n",
    "# Get a prediction\n",
    "pred = model.predict(X_new)\n",
    "\n",
    "# The model returns an array of predictions - one for each set of features submitted\n",
    "# In our case, we only submitted one patient, so our prediction is the first one in the resulting array.\n",
    "print('Predicted class is {}'.format(pred[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification\n",
    "\n",
    "Binary classification techniques work well when the data observations belong to one of two classes or categories, such as \"True\" or \"False\". When the data can be categorized into more than two classes, you must use a multiclass classification algorithm.\n",
    "\n",
    "Fortunately, in most machine learning frameworks, including scikit-learn, implementing a multiclass classifier is not significantly more complex than binary classification - and in many cases, the classification algorithm classes used for binary classification implicitly support multiclass classification.\n",
    "\n",
    "### Explore the data\n",
    "\n",
    "Let's start by examining a dataset that contains observations of multiple classes. We'll use a dataset that contains observations of three different species of penguin.\n",
    "\n",
    "> **Citation**: The penguins dataset used in the this exercise is a subset of data collected and made available by [Dr. Kristen\n",
    "Gorman](https://www.uaf.edu/cfos/people/faculty/detail/kristen-gorman.php)\n",
    "and the [Palmer Station, Antarctica LTER](https://pal.lternet.edu/), a\n",
    "member of the [Long Term Ecological Research\n",
    "Network](https://lternet.edu/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the training dataset\n",
    "penguins = pd.read_csv('data/penguins.csv')\n",
    "\n",
    "# Display a random sample of 10 observations\n",
    "sample = penguins.sample(10)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following columns:\n",
    "* **CulmenLength**: The length in mm of the penguin's culmen (bill).\n",
    "* **CulmenDepth**: The depth in mm of the penguin's culmen.\n",
    "* **FlipperLength**: The length in mm of the penguin's flipper.\n",
    "* **BodyMass**: The body mass of the penguin in grams.\n",
    "* **Species**: An integer value that represents the species of the penguin.\n",
    "\n",
    "The **Species** column is the label we want to train a model to predict. The dataset includes three possible species, which are encoded as 0, 1, and 2. The actual species names are revealed by the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "penguin_classes = ['Amelie', 'Gentoo', 'Chinstrap']\n",
    "print(sample.columns[0:5].values, 'SpeciesName')\n",
    "for index, row in penguins.sample(10).iterrows():\n",
    "    print('[',row[0], row[1], row[2], row[3], int(row[4]),']',penguin_classes[int(row[4])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what the feaures and labels in the data represent, let's explore the dataset. First, let's see if there are any missing (*null*) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count the number of null values for each column\n",
    "penguins.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are some missing feature values, but no missing labels. Let's dig a little deeper and see the rows that contain nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show rows containing nulls\n",
    "penguins[penguins.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two rows that contain no feature values at all (*NaN* stands for \"not a number\"), so these won't be useful in training a model. Let's discard them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows containing NaN values\n",
    "penguins=penguins.dropna()\n",
    "#Confirm there are now no nulls\n",
    "penguins.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've dealt with the missing values, let's explore how the features relate to the label by creating some box charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "penguin_features = ['CulmenLength','CulmenDepth','FlipperLength','BodyMass']\n",
    "penguin_label = 'Species'\n",
    "for col in penguin_features:\n",
    "    penguins.boxplot(column=col, by=penguin_label, figsize=(6,6))\n",
    "    plt.title(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the box plots, it looks like species 0 and 2 (Amelie and Chinstrap) have similar data profiles for culmen depth, flipper length, and body mass, but Chinstraps tend to have longer culmens. Species 1 (Gentoo) tends to have fairly clearly differentiated features from the others; which should help us train a good classification model.\n",
    "\n",
    "### Prepare the data\n",
    "\n",
    "Just as for binary classification, before training the model, we need to separate the features and label, and then split the data into subsets for training and validation. We'll also apply a *stratification* technique when splitting the data to maintain the proportion of each label value in the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and labels\n",
    "penguins_X, penguins_y = penguins[penguin_features].values, penguins[penguin_label].values\n",
    "\n",
    "# Split data 70%-30% into training set and test set\n",
    "x_penguin_train, x_penguin_test, y_penguin_train, y_penguin_test = train_test_split(penguins_X, penguins_y,\n",
    "                                                                                    test_size=0.30,\n",
    "                                                                                    random_state=0,\n",
    "                                                                                    stratify=penguins_y)\n",
    "\n",
    "print ('Training Set: %d, Test Set: %d \\n' % (x_penguin_train.size, x_penguin_test.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate a multiclass classifier\n",
    "\n",
    "Now that we have a set of training features and corresponding training labels, we can fit a multiclass classification algorithm to the data to create a model. Most scikit-learn classification algorithms inherently supports multiclass classification. We'll try a logistic regression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set regularization rate\n",
    "reg = 0.1\n",
    "\n",
    "# train a logistic regression model on the training set\n",
    "multi_model = LogisticRegression(C=1/reg).fit(x_penguin_train, y_penguin_train)\n",
    "print (multi_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the trained model to predict the labels for the test features, and compare the predicted labels to the actual labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "penguin_predictions = multi_model.predict(x_penguin_test)\n",
    "print('Predicted labels: ', penguin_predictions[:15])\n",
    "print('Actual labels   : ' ,y_penguin_test[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn. metrics import classification_report\n",
    "\n",
    "print(classification_report(y_penguin_test, penguin_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with binary classification, the report includes *precision* and *recall* metrics for each class. However, while with binary classification we could focus on the scores for the *positive* class; in this case, there are multiple classes so we need to look at an overall metric (either the macro or weighted average) to get a sense of how well the model performs across all three classes.\n",
    "\n",
    "You can get the overall metrics separately from the report using the scikit-learn metrics score classes, but with multiclass results you must specify which average metric you want to use for precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "print(\"Overall Accuracy:\",accuracy_score(y_penguin_test, penguin_predictions))\n",
    "print(\"Overall Precision:\",precision_score(y_penguin_test, penguin_predictions, average='macro'))\n",
    "print(\"Overall Recall:\",recall_score(y_penguin_test, penguin_predictions, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the confusion matrix for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Print the confusion matrix\n",
    "mcm = confusion_matrix(y_penguin_test, penguin_predictions)\n",
    "print(mcm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows the intersection of predicted and actual label values for each class - in simple terms, the diagonal intersections from top-left to bottom-right indicate the number of correct predictions.\n",
    "\n",
    "When dealing with multiple classes, it's generally more intuitive to visualize this as a heat map, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(mcm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(penguin_classes))\n",
    "plt.xticks(tick_marks, penguin_classes, rotation=45)\n",
    "plt.yticks(tick_marks, penguin_classes)\n",
    "plt.xlabel(\"Predicted Species\")\n",
    "plt.ylabel(\"True Species\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The darker squares in the confusion matrix plot indicate high numbers of cases, and you can hopefully see a diagonal line of darker squares indicating cases where the predicted and actual label are the same.\n",
    "\n",
    "### Preprocess data in a pipeline\n",
    "\n",
    "Again, just like with binary classification, you can use a pipeline to apply preprocessing steps to the data before fitting it to an algorithm to train a model. Let's see if we can improve the penguin predictor by scaling the numeric features in a transformation steps before training. We'll also try a different algorithm (a support vector machine), just to show that we can!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define preprocessing for numeric columns (scale them)\n",
    "feature_columns = [0,1,2,3]\n",
    "feature_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "# Create preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('preprocess', feature_transformer, feature_columns)])\n",
    "\n",
    "# Create training pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('regressor', SVC())])\n",
    "                           #('regressor', LogisticRegression(C=1/reg))])\n",
    "\n",
    "\n",
    "# fit the pipeline to train a linear regression model on the training set\n",
    "multi_model = pipeline.fit(x_penguin_train, y_penguin_train)\n",
    "print (multi_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get predictions from test data\n",
    "penguin_predictions = multi_model.predict(x_penguin_test)\n",
    "\n",
    "# Overall metrics\n",
    "print(\"Overall Accuracy:\",accuracy_score(y_penguin_test, penguin_predictions))\n",
    "print(\"Overall Precision:\",precision_score(y_penguin_test, penguin_predictions, average='macro'))\n",
    "print(\"Overall Recall:\",recall_score(y_penguin_test, penguin_predictions, average='macro'))\n",
    "\n",
    "# Confusion matrix\n",
    "plt.imshow(mcm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(penguin_classes))\n",
    "plt.xticks(tick_marks, penguin_classes, rotation=45)\n",
    "plt.yticks(tick_marks, penguin_classes)\n",
    "plt.xlabel(\"Predicted Species\")\n",
    "plt.ylabel(\"True Species\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model with new data observations\n",
    "\n",
    "Now let's save our trained model so we can use it again later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model as a pickle file\n",
    "filename = './models/penguin_model.pkl'\n",
    "joblib.dump(multi_model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so now we have a trained model. Let's use it to predict the class of a new penguin observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the model from the file\n",
    "multi_model = joblib.load(filename)\n",
    "\n",
    "# The model accepts an array of feature arrays (so you can predict the classes of multiple penguin observations in a single call)\n",
    "# We'll create an array with a single array of features, representing one penguin\n",
    "x_new = [[50.4,15.3,224,5550]]\n",
    "print ('New sample: {}'.format(x_new[0]))\n",
    "\n",
    "# The model returns an array of predictions - one for each set of features submitted\n",
    "# In our case, we only submitted one penguin, so our prediction is the first one in the resulting array.\n",
    "penguin_pred = multi_model.predict(x_new)[0]\n",
    "print('Predicted class is', penguin_classes[penguin_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also submit a batch of penguin observations to the model, and get back a prediction for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This time our input is an array of two feature arrays\n",
    "x_new = [[49.5,18.4,195, 3600],\n",
    "         [38.2,20.1,190,3900]]\n",
    "print ('New sample: {}'.format(x_new))\n",
    "\n",
    "# Call the web service, passing the input data\n",
    "predictions = multi_model.predict(x_new)\n",
    "\n",
    "# Get the predicted classes.\n",
    "for prediction in predictions:\n",
    "    print(prediction, '(' + penguin_classes[prediction] +')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn More\n",
    "\n",
    "Classification is one of the most common forms of machine learning, and by following the basic principles we've discussed in this notebook you should be able to train and evaluate classification models with scikit-learn. It's worth spending some time investigating classification algorithms in more depth, and a good starting point is the [Scikit-Learn documentation](https://scikit-learn.org/stable/user_guide.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonml",
   "language": "python",
   "name": "pythonml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
